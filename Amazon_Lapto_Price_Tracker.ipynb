{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYPpo/9b+h7KTKTwwVPqBc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## INSTALL REQUIRED LIBRARIES\n",
        "\n",
        "pip install requests beautifulsoup4 pandas openpyxl schedule lxml fake_useragent"
      ],
      "metadata": {
        "id": "85agZEG3f7fK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SCRAPE LAPTOP NAMES AND PRICES\n",
        "\n",
        "import requests   # To fetch the webpage\n",
        "from bs4 import BeautifulSoup   # To parse HTML\n",
        "import pandas as pd   # To handle data\n",
        "from fake_useragent import UserAgent   # To avoid getting blocked by Amazon\n",
        "\n",
        "# Amazon search results page for laptops\n",
        "url = \"https://www.amazon.com/s?k=laptops\"\n",
        "\n",
        "# Function to scrape product names and prices\n",
        "def scrape_amazon():\n",
        "    headers = {\"User-Agent\": UserAgent().random}  # Generate a random user-Agent\n",
        "    response = requests.get(url, headers=headers) # send requests to Amazon\n",
        "\n",
        "    if response.status_code == 200: # If request is successful\n",
        "        soup = BeautifulSoup(response.text, 'lxml') # Parse HTML using lxml\n",
        "        products = soup.find_all('div', {'data-component-type': 's-search-result'}) # Find product divs\n",
        "        data = []   #List to store extracted data\n",
        "\n",
        "        for product in products:\n",
        "          try:\n",
        "            # Extract product name\n",
        "            name = product.find('span', class_='a-size-medium').text.strip()\n",
        "\n",
        "            # Extract price (some products may not have a price)\n",
        "            price_whole = product.find('span', class_='a-price-whole')  # Find whole part of price\n",
        "            price_fraction = product.find('span', class_='a-price-fraction')  # Find cents part\n",
        "\n",
        "            if price_whole and price_fraction:  # If both parts are found\n",
        "                price = f\"{price_whole.text}{price_fraction.text}\"  # Combine whole and fraction\n",
        "\n",
        "            else:\n",
        "              price = \"N/A\"   # If no price is found\n",
        "\n",
        "            data.append([name, price])   # Append extracted data\n",
        "\n",
        "          except AttributeError:\n",
        "            continue  # Skip product if there's missing data\n",
        "\n",
        "        return data # Return extracted data\n",
        "    else:\n",
        "        print(\"Failed to fetch Amazon page\")\n",
        "        return[]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xk9OQrtkgb_d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SAVE DATA TO AN EXCEL FILE\n",
        "\n",
        "import openpyxl   # To handle Excel files\n",
        "\n",
        "# Function to save extracted data into an Excel file\n",
        "def save_to_excel(data):\n",
        "    df = pd.DateFrame(data, columns=['Product', 'Price']) # Convert data to DataFrame\n",
        "    df.to_excel('amazon_laptop_prices.xlsx', index=False) # Save to Excel\n",
        "    print(\"Data saved to Amazon_laptop_prices.xlsx\")  # Confirmation message\n",
        "\n"
      ],
      "metadata": {
        "id": "8_rIKc7Btfkp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## AUTOMATE DAILY PRICE TRACKING\n",
        "\n",
        "import schedule   # To automate task scheduling\n",
        "import time    # To keep the script running\n",
        "\n",
        "# Function to run the full process\n",
        "def run_price_tracker():\n",
        "  print(\"fetching latest laptop prices from Amazon...\")   # Log message\n",
        "  data = scrape_amazon()  # Scrape Amazon page\n",
        "  if data:\n",
        "    save_to_excel(data)  # Save data if scraping was successful\n",
        "  else:\n",
        "    print(\"No data extracted. Skipping save step.\")   # Handle errors\n",
        "\n",
        "# Schedule the script to run daily at 9:00am\n",
        "schedule.every().day.at(\"09:00\").do(run_price_tracker)\n",
        "\n",
        "# Keep the script running\n",
        "while True:\n",
        "  schedule.run_pending()  #check if it's time to run\n",
        "  time.sleep(60)  # wait 1 minute before checking again\n",
        "\n"
      ],
      "metadata": {
        "id": "Rr8NLWhCy6cF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}