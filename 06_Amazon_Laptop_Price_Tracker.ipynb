{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO4E8S0QnPibQLPe8nfH5C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## INSTALL REQUIRED LIBRARIES\n",
        "\n",
        "pip install requests beautifulsoup4 pandas openpyxl schedule lxml fake_useragent"
      ],
      "metadata": {
        "id": "85agZEG3f7fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#API_KEY = \"xxxxxxxxxxxxxxxxxxxxx\"\n",
        "#url = \"https://www.amazon.com/s?k=laptops\"\n",
        "\n",
        "# Use ScraperAPI to bypass Amazonâ€™s blocks\n",
        "#proxy_url = f\"http://api.scraperapi.com?api_key={API_KEY}&url={url}\"\n",
        "\n",
        "#response = requests.get(proxy_url)\n",
        "\n",
        "#print(response.text)  # Check if the HTML is retrieved successfully"
      ],
      "metadata": {
        "id": "eYej0W2n7mQL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SCRAPE LAPTOP NAMES AND PRICES\n",
        "\n",
        "import requests   # To fetch the webpage\n",
        "from bs4 import BeautifulSoup   # To parse HTML\n",
        "import pandas as pd   # To handle data\n",
        "from fake_useragent import UserAgent   # To avoid getting blocked by Amazon\n",
        "\n",
        "# Amazon search results page for laptops\n",
        "url = \"https://www.amazon.com/s?k=laptops\"\n",
        "\n",
        "# Function to scrape product names and prices\n",
        "def scrape_amazon():\n",
        "    headers = {\"User-Agent\": UserAgent().random}  # Generate a random user-Agent\n",
        "    response = requests.get(url, headers=headers) # send requests to Amazon\n",
        "\n",
        "    if response.status_code == 200: # If request is successful\n",
        "        soup = BeautifulSoup(response.text, 'lxml') # Parse HTML using lxml\n",
        "        products = soup.find_all('div', {'data-component-type': 's-search-result'}) # Find product divs\n",
        "        data = []   #List to store extracted data\n",
        "\n",
        "        for product in products:\n",
        "          try:\n",
        "            # Extract product name\n",
        "            name = product.find('span', class_='a-size-medium').text.strip()\n",
        "\n",
        "            # Extract price (some products may not have a price)\n",
        "            price_whole = product.find('span', class_='a-price-whole')  # Find whole part of price\n",
        "            price_fraction = product.find('span', class_='a-price-fraction')  # Find cents part\n",
        "\n",
        "            if price_whole and price_fraction:  # If both parts are found\n",
        "                price = f\"{price_whole.text}{price_fraction.text}\"  # Combine whole and fraction\n",
        "\n",
        "            else:\n",
        "              price = \"N/A\"   # If no price is found\n",
        "\n",
        "            data.append([name, price])   # Append extracted data\n",
        "\n",
        "          except AttributeError:\n",
        "            continue  # Skip product if there's missing data\n",
        "\n",
        "        return data # Return extracted data\n",
        "    else:\n",
        "        print(\"Failed to fetch Amazon page\")\n",
        "        return[]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xk9OQrtkgb_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SAVE DATA TO AN EXCEL FILE\n",
        "\n",
        "import openpyxl   # To handle Excel files\n",
        "\n",
        "# Function to save extracted data into an Excel file\n",
        "def save_to_excel(data):\n",
        "    df = pd.DateFrame(data, columns=['Product', 'Price']) # Convert data to DataFrame\n",
        "    df.to_excel('amazon_laptop_prices.xlsx', index=False) # Save to Excel\n",
        "    print(\"Data saved to Amazon_laptop_prices.xlsx\")  # Confirmation message\n",
        "\n"
      ],
      "metadata": {
        "id": "8_rIKc7Btfkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## AUTOMATE DAILY PRICE TRACKING\n",
        "\n",
        "import schedule   # To automate task scheduling\n",
        "import time    # To keep the script running\n",
        "\n",
        "# Function to run the full process\n",
        "#def run_price_tracker():\n",
        "  #print(\"fetching latest laptop prices from Amazon...\")   # Log message\n",
        "  #data = scrape_amazon()  # Scrape Amazon page\n",
        "  #if data:\n",
        "    #save_to_excel(data)  # Save data if scraping was successful\n",
        "  #else:\n",
        "    #print(\"No data extracted. Skipping save step.\")   # Handle errors\n",
        "\n",
        "# Function to scrape Amazon\n",
        "def scrape_amazon():\n",
        "  print(\"Fetching latest laptop prices from Amazon...\")\n",
        "  return [\n",
        "      {\"Product\": \"Apple MacBook Air M1\", \"Price\": \"$899\"},\n",
        "      {\"Product\": \"ASUS ROG Gaming Laptop\", \"Price\": \"$1,499\"},\n",
        "      {\"Product\": \"HP Pavillion 15\", \"Price\": \"$699\"},\n",
        "      {\"Product\": \"DELL XPS 13\", \"Price\": \"$1,199\"}\n",
        "  ]  # Mocked output (Replace with actual scraping logic)\n",
        "\n",
        "# Function to save data to Excel (need to define this function)\n",
        "def save_to_excel(data):\n",
        "  print(\"Saving data to Excel...\")  #Placeholder (will Replaced  with actual Excel writing logic)\n",
        "  for item in data:\n",
        "    print(f\"{item['Product']} - {item['Price']}\") # Print output immediately\n",
        "\n",
        "# Function to run the full process **immediately**\n",
        "def run_price_tracker():\n",
        "    data = scrape_amazon()  # Scrape Amazon page\n",
        "    if data:\n",
        "      save_to_excel(data)  # Save and print data immediately\n",
        "    else:\n",
        "      print(\"No data extracted. Skipping save step.\")  # Handle errors\n",
        "\n",
        "# Run the script immediately **without waiting**\n",
        "run_price_tracker()\n",
        "\n",
        "\n",
        "# Schedule the script to run daily at 9:00am\n",
        "#schedule.every().day.at(\"09:00\").do(run_price_tracker)\n",
        "\n",
        "# Keep the script running\n",
        "#while True:\n",
        "  #schedule.run_pending()  #check if it's time to run\n",
        "  #time.sleep(60)  # wait 1 minute before checking again\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr8NLWhCy6cF",
        "outputId": "b07fca4c-3c11-4608-c13e-2fb4e524ab0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching latest laptop prices from Amazon...\n",
            "Saving data to Excel...\n",
            "Apple MacBook Air M1 - $899\n",
            "ASUS ROG Gaming Laptop - $1,499\n",
            "HP Pavillion 15 - $699\n",
            "DELL XPS 13 - $1,199\n"
          ]
        }
      ]
    }
  ]
}